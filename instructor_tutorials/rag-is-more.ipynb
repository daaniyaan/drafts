{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG is more than just embedding search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a> \n",
    "\n",
    "- [Introduction](#toc2_)\n",
    "    - [What is RAG?](#toc3_)\n",
    "    - [How do RAG models work?](#toc4_)\n",
    "    - [Why there is a need for them?](#toc5_)\n",
    "- [The 'Dumb' RAG Model](#toc6_)\n",
    "    - [What is it?](#toc7_)\n",
    "    - [What are the limitations?](#toc8_)\n",
    "- [Improving the RAG Model](#toc9_)\n",
    "- [Practical Examples](#toc10_)\n",
    "    - [Case Study: Metaphor Systems](#toc11_)\n",
    "    - [Case Study: Personal Assistant](#toc12_)\n",
    "- [Utilizing Frameworks and Libraries](#toc13_)\n",
    "- [Conclusion](#toc14_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a> [Introduction](#toc0_)\n",
    "\n",
    "<a id='toc3_'></a>**What is RAG?**\n",
    "\n",
    "Retrieval Augmented Generation (RAG) models are the bridge between large language models and external knowledge databases. They fetch the relevant data for a given query. For example, if you have some documents and want to ask questions related to the content of those documents, RAG models help by retrieving data from those documents and passing it to the LLM in queries.\n",
    "\n",
    "<a id='toc4_'></a>**How do RAG models work?**\n",
    "\n",
    "The typical RAG process involves embedding a user query and searching a vector database to find the most relevant information to supplement the generated response. This approach is particularly effective when the database contains information closely matching the query but not more than that.\n",
    "\n",
    "![Image](https://jxnl.github.io/instructor/blog/img/dumb_rag.png)\n",
    "\n",
    "\n",
    "<a id='toc5_'></a>**Why is there a need for them?**\n",
    "\n",
    "\n",
    "Pre-trained large language models do not learn over time. If you ask them a question they have not been trained on, they will often hallucinate. Therefore, we need to embed our own data to achieve a better output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>[The 'Dumb' RAG Model](#toc0_)\n",
    "\n",
    "<a id='toc7_'></a>**What is it?**\n",
    "\n",
    "The <u>simplest</u> implementation of RAG embeds a user query and do a <u>straightforward</u> search in a database, like a vector store of Wikipedia articles.\n",
    "\n",
    "this approach often falls short when dealing with complex queries and diverse data sources.\n",
    "\n",
    "<a id='toc8_'></a>**What are the limitations?**\n",
    "\n",
    "This Simple RAG model suffers from several limitations:\n",
    "- **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.\n",
    "    - Query: \"Tell me about climate change effects on marine life.\"\n",
    "    - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics.\n",
    "<!-- blank -->\n",
    "- **Monolithic Search Backend:** It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources.\n",
    "    - Query: \"Latest research in quantum computing.\"\n",
    "    - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources.\n",
    "<!-- blank -->\n",
    "- **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.\n",
    "    - Query: \"what problems did we fix last week\"\n",
    "    - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week.\n",
    "<!-- blank -->\n",
    "- **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.\n",
    "    - Query: \"Tips for first-time Europe travelers.\"\n",
    "    - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc9_'></a>[Improving the RAG model](#toc0_)\n",
    "\n",
    "**What's the solution?**\n",
    "\n",
    "Enhancing RAG requires a more sophisticated approach known as query understanding.\n",
    "\n",
    "This process involves analyzing the user's query and transforming it to better match the backend's search capabilities.\n",
    "\n",
    "By doing so, we can significantly improve both the precision and recall of the search results, providing more accurate and relevant responses.\n",
    "\n",
    "![Image](https://jxnl.github.io/instructor/blog/img/query_understanding.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc10_'></a>[Practical Examples](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples below, we're going to use the [`instructor`](https://github.com/jxnl/instructor) library to simplify the interaction between the programmer and language models via the function-calling API.\n",
    "\n",
    "<small>some of these case studied below has been inspired/done in collab with a few of my clients at new.computer, Metaphor system and Naro. Check them out.</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc11_'></a>Case Study: Metaphor Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metaphor Systems presents an advanced implementation of the RAG model. It employs a unique approach to transform natural language queries into their custom search-optimized queries.\n",
    "\n",
    "If you take a look at their web UI, you'll notice an auto-prompt option. This feature uses function calls to further optimize your query using a language model. The result is a fully specified Metaphor Systems query, optimized for their custom search.\n",
    "\n",
    "This approach not only enhances the precision of the search results but also improves the overall user experience by providing more accurate and relevant responses.\n",
    "\n",
    "![Image](https://jxnl.github.io/instructor/blog/img/meta.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in.\n",
    "\n",
    "It's actually more complex than this but this is a good start.\n",
    "\n",
    "We can model this structured output in Pydantic using the [`instructor`](https://github.com/jxnl/instructor) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import datetime\n",
    "from typing import List\n",
    "\n",
    "class DateRange(BaseModel):\n",
    "    \"\"\"\n",
    "    A Pydantic model that represents a date range.\n",
    "    \"\"\"\n",
    "    start: datetime.date\n",
    "    end: datetime.date\n",
    "\n",
    "class MetaphorQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    A Pydantic model that represents a Metaphor Systems query.\n",
    "    \"\"\"\n",
    "    rewritten_query: str\n",
    "    published_daterange: DateRange\n",
    "    domains_allow_list: List[str]\n",
    "\n",
    "    async def execute():\n",
    "        \"\"\"\n",
    "        An asynchronous method that executes the query.\n",
    "        \"\"\"\n",
    "        # Logic to interact with Metaphor Systems' search backend\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `DateRange` and `MetaphorQuery` are Pydantic models that structure the user's query with a date range and a list of domains to search within.\n",
    "\n",
    "These models **restructure** the user's query by including a <u>rewritten query</u>, a <u>range of published dates</u>, and a <u>list of domains</u> to search in.\n",
    "\n",
    "This approach enhances the performance of the search backend without requiring the user to understand its intricacies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the new restructured query, we can apply this pattern to our function calls to obtain results that are optimized for our backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"rewritten_query\": \"recent developments in AI\",\n",
      "    \"published_daterange\": {\n",
      "        \"start\": \"2023-01-01\",\n",
      "        \"end\": \"2023-11-10\"\n",
      "    },\n",
      "    \"domains_allow_list\": [\n",
      "        \"technology\",\n",
      "        \"artificial intelligence\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import instructor \n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Fetch the OpenAI API key from the environment variables. If not found, replace it with 'YOUR API KEY'.\n",
    "api__key = os.getenv('OPENAI_API_KEY', \"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\")\n",
    "\n",
    "# Enables response_model in the openai client\n",
    "client = instructor.patch(OpenAI(api_key=api__key))\n",
    "\n",
    "query = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    response_model=MetaphorQuery,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You're a query understanding system for the Metafor Systems search engine. Today is Nov10 2023. Here are some tips: ...\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"What are some recent developments in AI?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(query.model_dump_json(indent=4)) # Printing the Json dump of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of this approach goes beyond just adding date ranges to your search.\n",
    "\n",
    "It's about creating nuanced, tailored searches that are deeply integrated with the backend.\n",
    "\n",
    "Metaphor Systems offers a comprehensive suite of filters and options that you can leverage to construct a powerful search query. \n",
    "\n",
    "they can even employ a chain of thought prompting mechanism to enhance the utilization of these advanced features, making your search more efficient and precise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"rewritten_query\": \"recent developments in AI\",\n",
      "    \"published_daterange\": {\n",
      "        \"start\": \"2022-11-10\",\n",
      "        \"end\": \"2023-11-10\",\n",
      "        \"chain_of_thought\": \"The user is asking for recent developments. Given today's date, the relevant timeframe for recent should be 1 year.\"\n",
      "    },\n",
      "    \"domains_allow_list\": [\n",
      "        \"Artificial Intelligence\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class DateRange(BaseModel):\n",
    "    start: datetime.date\n",
    "    end: datetime.date\n",
    "    chain_of_thought: str = Field( # Chain of thought prompting \n",
    "        None,\n",
    "        description=\"Think step by step to plan what is the best time range to search in\"\n",
    "    )\n",
    "\n",
    "class MetaphorQuery(BaseModel):\n",
    "    rewritten_query: str\n",
    "    published_daterange: DateRange\n",
    "    domains_allow_list: List[str]\n",
    "\n",
    "    async def execute():\n",
    "        ...\n",
    "\n",
    "query = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    response_model=MetaphorQuery,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You're a query understanding system for the Metafor Systems search engine. Today is Nov10 2023. Here are some tips: ...\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"What are some recent developments in AI?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(query.model_dump_json(indent=4)) # Printing the Json dump of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc12_'></a>Case Study: Personal Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A personal assistant application needs to interpret vague queries and fetch information from multiple backends, such as emails and calendars. By modeling the assistant's capabilities using Pydantic, we can dispatch the query to the correct backend and retrieve a unified response.\n",
    "\n",
    "For instance, when you ask, \"What's on my schedule today?\", the application needs to fetch data from various sources like events, emails, and reminders. This data is stored across different backends, but the goal is to provide a consolidated summary of results.\n",
    "\n",
    "It's important to note that the data from these sources may not be embedded in a search backend. Instead, they could be accessed through different clients like a calendar or email, spanning both personal and professional accounts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "import datetime\n",
    "import asyncio\n",
    "\n",
    "# The ClientSource Enum represents the different sources from which data can be fetched.\n",
    "class ClientSource(enum.Enum):\n",
    "    GMAIL = \"gmail\"\n",
    "    CALENDAR = \"calendar\"\n",
    " \n",
    "# a Pydantic model that represents a search client.\n",
    "# It contains the query, keywords, email, source, and date range for the search.\n",
    "class SearchClient(BaseModel):\n",
    "    query: str\n",
    "    keywords: List[str]\n",
    "    email: str\n",
    "    source: ClientSource\n",
    "    start_date: datetime.date\n",
    "    end_date: datetime.date\n",
    " \n",
    "    # an asynchronous method that fetches data from the specified source.\n",
    "    async def execute(self) -> str:\n",
    "        if self.source == ClientSource.GMAIL:\n",
    "            # Fetch data from Gmail\n",
    "            ...\n",
    "        elif self.source == ClientSource.CALENDAR:\n",
    "            # Fetch data from Calendar\n",
    "            ...\n",
    " \n",
    "# a Pydantic model that represents a retrieval operation.\n",
    "# It contains a list of queries to be executed.\n",
    "class Retrival(BaseModel):\n",
    "    queries: List[SearchClient]\n",
    "\n",
    "    # The execute method is an asynchronous method that executes all the queries and return the results.\n",
    "    async def execute(self) -> str:\n",
    "        return await asyncio.gather(*[query.execute() for query in self.queries])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can utilize this with a straightforward query such as \"What do I have today?\".\n",
    "\n",
    "The system will attempt to asynchronously dispatch the query to the appropriate backend.\n",
    "\n",
    "However, it's still crucial to remember that effectively prompting the language model is still a key aspect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"queries\": [\n",
      "        {\n",
      "            \"query\": \"Jason's schedule\",\n",
      "            \"keywords\": [\n",
      "                \"meeting\",\n",
      "                \"event\",\n",
      "                \"appointment\"\n",
      "            ],\n",
      "            \"email\": \"jason@example.com\",\n",
      "            \"source\": \"calendar\",\n",
      "            \"start_date\": \"2023-11-10\",\n",
      "            \"end_date\": \"2023-11-10\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "retrival = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    response_model=Retrival,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are Jason's personal assistant. Today is November 10 2023.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What do I have today?\"}\n",
    "    ],\n",
    ")\n",
    "print(retrival.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more challenging, we will assign it multiple tasks, followed by a list of queries that are routed to various search backends, such as email and calendar. Not only do we dispatch to different backends, over which we have no control, but we are also likely to render them to the user in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"queries\": [\n",
      "        {\n",
      "            \"query\": \"Jason's meetings\",\n",
      "            \"keywords\": [\n",
      "                \"meeting\",\n",
      "                \"appointment\"\n",
      "            ],\n",
      "            \"email\": \"Jason's email\",\n",
      "            \"source\": \"calendar\",\n",
      "            \"start_date\": \"2023-11-10\",\n",
      "            \"end_date\": \"2023-11-10\"\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"Jason's emails\",\n",
      "            \"keywords\": [\n",
      "                \"important\",\n",
      "                \"urgent\",\n",
      "                \"ASAP\"\n",
      "            ],\n",
      "            \"email\": \"Jason's email\",\n",
      "            \"source\": \"gmail\",\n",
      "            \"start_date\": \"2023-11-10\",\n",
      "            \"end_date\": \"2023-11-10\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "retrival = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    response_model=Retrival,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are Jason's personal assistant. Today is November 10 2023.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What meetings do I have today and are there any important emails I should be aware of?\"}\n",
    "    ],\n",
    ")\n",
    "print(retrival.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc13_'></a>[Utilizing Frameworks and Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these examples showcase how both search providors and consumers can use [`instructor`](https://github.com/jxnl/instructor) to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend.\n",
    "\n",
    "We can extend our RAG model's functionality by integrating various frameworks and libraries. This could include user interaction prompts, external data fetches, and more, all within the dispatch process. This modularity allows developers to build custom solutions that cater to their specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc14_'></a>[Conclusion](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial has introduced you to advanced techniques to improve RAG models by incorporating query understanding and structuring queries using Pydantic and instructor. The aim is to move beyond the limitations of the 'Dumb' RAG model and toward a more intelligent system capable of handling complex queries with context-aware responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
